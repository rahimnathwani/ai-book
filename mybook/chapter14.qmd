# Cost, Latency, and Operational Concerns {#sec-cost-latency}

*The practical economics of agent systems*

Agents are powerful, but they're not free. Every LLM call costs money. Every tool execution takes time. At scale, these costs add up—sometimes dramatically.

This chapter explores the practical economics and performance characteristics of agent systems: what they cost, why latency matters, when the economics work in your favor, and how to optimize for both cost and performance.

## Understanding Costs

Agent systems have different cost structures than traditional software.

### Per-Token Pricing

Most LLM APIs charge based on tokens processed:

**Input tokens:** Text sent to the model (prompts, conversation history, tool results)
**Output tokens:** Text generated by the model (responses, reasoning)

**Typical pricing (as of 2024):**
```
GPT-4:
  Input: $0.03 per 1K tokens
  Output: $0.06 per 1K tokens

Claude Sonnet:
  Input: $0.003 per 1K tokens
  Output: $0.015 per 1K tokens

GPT-3.5:
  Input: $0.0005 per 1K tokens
  Output: $0.0015 per 1K tokens
```

### Cost Per Agent Task

A simple task might use:
- System prompt: 500 tokens
- User request: 100 tokens
- Agent reasoning: 200 tokens
- Tool calls and results: 1,000 tokens
- Final response: 200 tokens

**Total: ~2,000 tokens**

At GPT-4 pricing:
- Input: 1,600 tokens × $0.03/1K = $0.048
- Output: 400 tokens × $0.06/1K = $0.024
- **Total: ~$0.07 per task**

Seems cheap! But:
- 1,000 tasks/day = $70/day = $2,100/month
- 10,000 tasks/day = $700/day = $21,000/month

### What Drives Costs

**1. Context window usage**
Larger contexts = more input tokens = higher cost

Example:
- Including 50KB document in every request
- 50KB ≈ 40,000 tokens
- At $0.03/1K: $1.20 just for the document
- 1,000 requests/day = $1,200/day for document inclusion alone

**2. Number of iterations**
Complex tasks require multiple LLM calls:
- Initial reasoning: 1 call
- Tool selection: 1 call
- Result interpretation: 1 call
- Next action decision: 1 call
- Final response: 1 call

5 calls instead of 1 = 5x cost

**3. Model choice**
GPT-4 vs GPT-3.5: ~60x price difference

**4. Output length**
Verbose responses cost more than concise ones

### Comparing to Human Time

Context matters:

**Example: Customer support**
- Agent cost per ticket: $0.10
- Human cost per ticket: $5-10 (15 minutes @ $20-40/hour)
- Agent is 50-100x cheaper

**Example: Data analysis**
- Agent cost: $0.50
- Human cost: $50-100 (1-2 hours @ $50/hour)
- Agent is 100-200x cheaper

**Even at scale:**
- 10,000 tickets/month
- Agent: $1,000
- Human: $50,000-100,000
- Savings: $49,000-99,000/month

The agent's value proposition remains strong despite non-zero cost.

## Managing Costs

Strategies to control expenses:

### 1. Choose Appropriate Models for Tasks

**Use cheaper models for simpler tasks:**

```python
def route_to_model(task):
    complexity = analyze_complexity(task)

    if complexity == "simple":
        return "gpt-3.5-turbo"  # Cheap, fast
    elif complexity == "medium":
        return "claude-sonnet"   # Balanced
    else:
        return "gpt-4"           # Expensive, capable
```

**Don't use GPT-4 for:**
- Simple classification
- Template filling
- Straightforward extraction
- Routine operations

**Do use GPT-4 for:**
- Complex reasoning
- Novel problem-solving
- High-stakes decisions
- Multi-step planning

### 2. Efficient Prompt Design

**Bad: Wasteful**
```
"Here's our entire 100-page employee handbook [50,000 tokens].
What's the vacation policy?"
```
Cost: $1.50 per query

**Good: Efficient**
```
[Use RAG to retrieve only vacation policy section - 500 tokens]
"Based on this policy section, what's the vacation policy?"
```
Cost: $0.02 per query

**75x cost reduction**

### 3. Caching

**Cache common contexts:**

Some providers offer prompt caching:
```python
# First call: Full cost
response = llm.complete(
    system_prompt=LARGE_SYSTEM_PROMPT,  # 10K tokens
    cache=True
)

# Subsequent calls: Cached system prompt costs 10% of original
response = llm.complete(
    system_prompt=LARGE_SYSTEM_PROMPT,  # Cached
    user_prompt=different_query
)
```

**90% savings on repeated context**

### 4. Batch Processing

**Instead of:**
```python
for item in 1000_items:
    result = agent.process(item)  # 1000 API calls
```

**Consider:**
```python
# Process in batches
batch_prompt = f"Process these items:\n{items_json}"
results = agent.process_batch(batch_prompt)  # 1 API call
```

**Caveat:** Only works for independent items that fit in context

### 5. Set Budget Limits

**Implement spending controls:**

```python
class BudgetController:
    def __init__(self, daily_limit_usd=100):
        self.daily_limit = daily_limit_usd
        self.today_spend = 0
        self.last_reset = date.today()

    def check_budget(self, estimated_cost):
        # Reset if new day
        if date.today() > self.last_reset:
            self.today_spend = 0
            self.last_reset = date.today()

        # Check if would exceed limit
        if self.today_spend + estimated_cost > self.daily_limit:
            raise BudgetExceededError(
                f"Would exceed daily limit of ${self.daily_limit}"
            )

        self.today_spend += estimated_cost

    def track_actual_cost(self, actual_cost):
        self.today_spend = actual_cost
```

### 6. Monitor and Alert

**Track costs in real-time:**

```python
# Log every API call cost
def track_api_call(model, input_tokens, output_tokens, cost):
    metrics.record({
        "timestamp": now(),
        "model": model,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "cost_usd": cost,
        "task_type": current_task_type
    })

# Alert on anomalies
if hourly_cost > baseline_hourly_cost * 3:
    alert("Unusual spike in API costs!")
```

## Latency Considerations

Agent tasks take time. Understanding latency is crucial for user experience.

### Where Time Goes

**Typical agent task breakdown:**

```
User submits request: 0ms
→ Request processing: 50ms
→ LLM call 1 (initial reasoning): 2,000ms
→ Tool execution 1: 300ms
→ LLM call 2 (interpret results): 1,500ms
→ Tool execution 2: 200ms
→ LLM call 3 (final response): 1,000ms
→ Response formatting: 50ms
Total: ~5,100ms (5.1 seconds)
```

**LLM calls dominate latency.**

### Factors Affecting LLM Latency

**1. Input length**
- Longer contexts take longer to process
- ~linear relationship

**2. Output length**
- Each token generated sequentially
- Longer outputs = more time
- Can limit with `max_tokens` parameter

**3. Model size**
- Larger models (GPT-4) slower than smaller (GPT-3.5)
- Tradeoff: capability vs. speed

**4. API load**
- Provider congestion affects response time
- Can vary throughout day

**5. Model parameters**
- Temperature, top_p affect generation speed slightly
- Structured output may be slower

### Managing Latency

**1. Set user expectations**

```
"Analyzing your data... This may take 10-15 seconds"
[Progress indicator]
```

**2. Stream responses**

```python
for chunk in llm.stream(prompt):
    print(chunk, end='', flush=True)
```

User sees progress, feels faster even if total time is same.

**3. Async patterns for long-running tasks**

```python
# Don't block user
task_id = agent.start_task_async(complex_task)
return {"task_id": task_id, "status": "processing"}

# User can check status
status = agent.get_task_status(task_id)
# Returns: {"status": "complete", "result": ...}
```

**4. Parallel tool execution**

```python
# Sequential (slow)
result1 = tool1.execute()
result2 = tool2.execute()
result3 = tool3.execute()
# Total: 900ms

# Parallel (fast)
results = await asyncio.gather(
    tool1.execute(),
    tool2.execute(),
    tool3.execute()
)
# Total: 300ms (limited by slowest)
```

**5. Cache aggressively**

```python
@cache(ttl=3600)  # Cache for 1 hour
def get_product_catalog():
    # Expensive API call
    return fetch_catalog()
```

**6. Use faster models when acceptable**

```python
if task_urgency == "high" and complexity == "low":
    model = "gpt-3.5-turbo"  # Fast, cheap
else:
    model = "gpt-4"  # Slower, better
```

## When the Economics Don't Work

Be honest about when agents aren't cost-effective:

### High-Volume, Low-Value Tasks

**Example: Classifying millions of simple items**

- Volume: 10M items/day
- Cost per item: $0.001 (very cheap model)
- Daily cost: $10,000
- Monthly cost: $300,000

Traditional ML model:
- Training cost: $5,000 (one-time)
- Inference cost: $100/month
- Amortized: Vastly cheaper

**When volume is huge and task is consistent, train a specialized model.**

### Tasks Requiring Very Low Latency

**Example: Real-time fraud detection**

- Required latency: <100ms
- LLM latency: 500-2000ms
- **Agent can't meet requirement**

Use traditional ML or rule-based system.

### When Traditional Software Is More Cost-Effective

**Example: Summing a column of numbers**

- Agent: $0.001-0.01 per operation
- Traditional code: $0.0000001 per operation
- **100-100,000x cheaper to write simple code**

Don't use agents for tasks traditional software handles easily and cheaply.

## Operational Best Practices

**1. Start small, measure, scale**
- Pilot with limited usage
- Measure actual costs and latency
- Extrapolate to full scale
- Optimize before full rollout

**2. Monitor continuously**
- Cost per task
- Latency distribution
- Error rates
- Token usage patterns

**3. Set up alerts**
- Cost spikes
- Latency degradation
- Error rate increases
- Budget approaching limit

**4. Optimize iteratively**
- Profile where time/cost goes
- Address biggest contributors
- Measure improvement
- Repeat

**5. Plan for growth**
- Costs scale with usage
- Budget for expected growth
- Have cost reduction strategies ready
- Monitor unit economics

## What We've Learned

- **Agents have real costs** (token-based pricing)
- **But often cheaper than human alternatives** at scale
- **Cost optimization is essential** for production deployment
- **Latency matters** for user experience
- **Sometimes agents aren't the right solution** economically
- **Monitoring and alerting are critical**

The economics of agents are favorable for many use cases, but require active management. The most capable system is worthless if it's too expensive or slow to run in production.

## Looking Ahead

Understanding costs and latency helps you build systems that are economically sustainable. But to truly operate agents in production, you need visibility into what they're doing.

The next chapter explores observability and debugging—understanding what your agent is thinking, why it made decisions, and how to diagnose problems when things go wrong.

Because you can't fix what you can't see.
